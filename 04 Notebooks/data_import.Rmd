---
title: "Import Coastal Water Quality Data"
author: "Francois van Schalkwyk"
date: "2025-03-19"
output: html_document
---

# Workspace Set-Up

Make sure that you are working in the outfalls project, found in the working directory.

## Set working directory for all code chunks to project root directory

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

## Load required packages

```{r message = FALSE}
library(tidyverse)
library(dbplyr)
library(DBI)
library(pdftools)
library(readxl)
```

## Database Connection

```{r db connection}
# Connect to the Postgres database
con <- dbConnect(RPostgres::Postgres(),
  host = "infinity.cdmqm0mu0qf8.eu-north-1.rds.amazonaws.com",
  port = 5432,
  user = rstudioapi::askForPassword(prompt = "Username: "),
  password = rstudioapi::askForPassword(),
  dbname = "infinity"
)
```

```{r}
dbGetInfo(con)
```

# Data Import

## Check which pdf files have not yet been imported

```{r not-yet-imported}
# List of files in folder
files_list <- list.files("../../Infinity/Coastal Water Quality - Documents/Shared Results/SSB CMB Monitoring/", pattern = "*.pdf")

extract_sample_date <- function(filename){
  sample_date <- pdf_text(paste("../../Infinity/Coastal Water Quality - Documents/Shared Results/SSB CMB Monitoring/", filename, sep = "")) |>
    str_split("\\n") |>
    unlist() |>
    keep(~str_detect(.x, "Sample taken")) |>
    str_extract("\\d{1,2}\\s[:alpha:]{3,12}\\s\\d{4}") |>
    pluck(1) |>
    dmy()
  
  tibble(filename = filename, sample_date = sample_date)
}

# Map the function across the list of files in the directory
files <- map_df(files_list, ~extract_sample_date(.x))

# Retrieve a list of sample dates from the results table in database
database <- tbl(con, I("coastal.results")) |>
  distinct(sample_date) |>
  collect()

# Check which files are not yet in database
files |>
  left_join(database, by = c("sample_date" = "sample_date"), keep = TRUE) |>
  filter(is.na(sample_date.y))
```

## Extract data from pdf

```{r extract-pdf}
# Select the pdf that you want to import
path <- "../../Infinity/Coastal Water Quality - Documents/Shared Results/SSB CMB Monitoring/CWQ_59(28.03.2025).pdf"
filename <- str_remove(path, "../../Infinity/Coastal Water Quality - Documents/Shared Results/SSB CMB Monitoring/")

# Extract the pdf data
pdf_data <- pdf_text(path) |>
  str_split("\n")

# Extract the sample date
sample_date <- pdf_data |>
  unlist() |> 
  keep(~str_detect(.x, "Sample taken")) |>
  str_extract("\\d{1,2}\\s[:alpha:]{3,12}\\s\\d{4}") |>
  pluck(1) |>
  dmy()

# Extract the analysis date
analysis_completed <- pdf_data |>
  unlist() |> 
  keep(~str_detect(.x, "Analysis completed")) |>
  str_extract("\\d{1,2}\\s[:alpha:]{3,12}\\s\\d{4}") |>
  pluck(1) |>
  dmy()

# Extract the sample temperature
sample_temp_c <- pdf_data |>
  unlist() |> 
  keep(~str_detect(.x, "Sample temperature:")) |>
  str_extract("(?<=(Sample temperature:\\s))\\d{1,2}\\.\\d") |>
  pluck(1) |>
  as.numeric()

# Extract tabular data
extract_table <- function(page){
  page |>
    unlist() |> 
    head_while(~!str_detect(.x, "colony forming units")) |>
    tail_while(~!str_detect(.x, "Sample number")) |>
    discard(~.x == "") |>
    unlist()
}

# Extract table from all pages of pdf
table <- map(pdf_data, ~extract_table(.x)) |>
  unlist()

# Extract sample number
sample_number <- table |>
  str_sub(5,15) |>
  str_trim()

# Extract sample text
sample_text <- table |>
  str_sub(19,95) |>
  str_trim()

# Extract result
sample_result <- table |>
  str_sub(100) |>
  str_trim()

# Make a table with the extracted data
tbl <- tibble(number = sample_number,
       text = sample_text,
       result = sample_result) |> 
  mutate(number = if_else(number == "", NA, number)) |>
  fill(number, .direction = "down") |> 
  group_by(number) |>
  summarise(text = str_c(text, collapse = ""), result = first(result)) |>
  select(text, result) |>
  mutate(
    site_id = str_extract(text, "[:upper:]{2,3}\\s?\\d{2}\\d?[AOIWETC]?") |> str_remove_all("\\s"),
    sample_time = str_extract(text, "\\d{1,2}h\\d{2}") |> str_replace("h", ":"),
    sample_date = sample_date,
    analysis_completed = analysis_completed,
    sample_temp_c = sample_temp_c,
    filename = filename
    ) |>
  select(-text) |>
  rename(enterococci_cfu_per_100ml = result) |>
  mutate(site_id = case_when(
    str_detect(site_id, "\\d{3}") & str_detect(site_id, "1$") ~ str_replace(site_id, "1$", "I"),
    str_detect(site_id, "\\d{3}") & str_detect(site_id, "0$") ~ str_replace(site_id, "0$", "O"),
    .default = site_id
  )) |>
  mutate(enterococci_cfu_per_100ml = str_replace(enterococci_cfu_per_100ml, "None found", "ND"))

# Copy to database
copy_to(con, tbl, name = "coastal", overwrite = TRUE)
```

## Upload to database

```{sql, connection = con}
BEGIN;
```

```{sql, connection = con}
WITH cte AS 
(SELECT
  site_id,
  sample_date,
  sample_time::time,
  analysis_completed,
  sample_temp_c,
  enterococci_cfu_per_100ml,
  filename,
  'WLAB'::coastal.lab AS lab_code
FROM coastal)
INSERT INTO coastal.results (site_id, sample_date, sample_time, analysis_completed, sample_temp_c, enterococci_cfu_per_100ml, filename, lab_code)
SELECT * FROM cte;
```

```{sql, connection = con}
SELECT
  site_id, sample_date, sample_time, enterococci_cfu_per_100ml
FROM coastal.results
WHERE sample_date = '2025-03-28';
```

```{sql, connection = con}
ROLLBACK;
```

```{sql, connection = con}
COMMIT;
```

# Historical Data

```{r list-files}
# List the files in the directory
path <- "01 Input Data/Coastal Water Quality Data/"
files <- list.files(path = path, pattern = "*.xls*")
full_path <- paste(path, files, sep = "")
```

```{r read-excel-function-copy}
# Create function for extracting data and populating table
read_bacteria_data <- function(file, sheet){
  # Extract filename
  filename <- file |> str_remove("01 Input Data/Coastal Water Quality Data/")
  
  # Read Ecoli data
  data <- read_excel(file,
                      range = "A1:B1000",
                      sheet = sheet,
                      col_names = c("date", "value_cfu_per_100ml"),
                      col_types = c("date", "numeric"))
  
  # If empty tibble, create new empty tibble
  if (nrow(data) == 0) {
    data <- tibble(date = date(), value_cfu_per_100ml = numeric())
    data <- add_case(data, date = NA, value_cfu_per_100ml = NA) |>
      mutate(date = date(date))
  }
  
  results <- data |>
    slice(-1) |>
    filter(!is.na(value_cfu_per_100ml)) |>
    mutate(
      date = date(date),
      file = filename
      )

  return(results)
}
```

```{r run-function}
# Run the function with all files for E. coli
ecoli <- str_c(path, files) |>
  map_df(~read_bacteria_data(.x, 2))

# Run the function for faecal streptococci
faecal <- str_c(path, files) |>
  map_df(~read_bacteria_data(.x, 3))
```

```{r summary-statistics}
# Check summary statistics for ecoli
ecoli |>
  group_by(file) |>
  arrange(file) |>
  summarise(
    min_date = min(date),
    max_date = max(date),
    n = sum(!is.na(value_cfu_per_100ml)),
    min = min(value_cfu_per_100ml, na.rm = TRUE),
    max = max(value_cfu_per_100ml, na.rm = TRUE)
    )

# Check summary statistics for faecal streptococci
faecal |>
  group_by(file) |>
  arrange(file) |>
  summarise(
    min_date = min(date),
    max_date = max(date),
    n = sum(!is.na(value_cfu_per_100ml)),
    min = min(value_cfu_per_100ml, na.rm = TRUE),
    max = max(value_cfu_per_100ml, na.rm = TRUE)
    )
```

```{r intensive-cleaning}
# For each file, calculate the gap between the most recent and next most recent file. If the value is 1, remove that record
ecoli |>
  group_by(file) |>
  mutate(gap = date - lag(date)) |>
  filter(!(gap > "100 days" & value_cfu_per_100ml == 1) & file != "64_Muizenberg_Sufing_area-In_Line_with_Bathing_area_(CS34) SSB and CMB.xlsm") |>
  ggplot(aes(x = date, y = value_cfu_per_100ml, colour = file)) +
  geom_point() +
  theme(legend.position = "none") +
  coord_cartesian(xlim = c(ymd("2024-01-01"), ymd("2025-07-01")), ylim = c(0, 250))

# Clean data
ecoli <- ecoli |>
  group_by(file) |>
  mutate(gap = date - lag(date)) |>
  filter(!(gap > "100 days" & value_cfu_per_100ml == 1)) |>
  filter(!(file == "56_Fish_Hoek-Corner_of_the_Beach,_at_Jager_Walk_(CS35) CMB and SSB.xlsm" & date > "2024-02-21" & value_cfu_per_100ml == 1)) |> 
  filter(!(file == "56_Fish_Hoek-Corner_of_the_Beach,_at_Jager_Walk_(CS35).xlsm" & date > "2024-02-21" & value_cfu_per_100ml == 1)) |>
  filter(!(file == "56_Fish_Hoek_lifesaving_club_(CS41).xlsm" & date > "2024-02-21" & value_cfu_per_100ml == 1)) |>
  filter(!(file == "57_Woolleys_Tidal_Pool_(CS42).xlsm" & date >= "2024-02-07" & round(value_cfu_per_100ml, 2) == 3.33)) |>
  filter(!(file == "59_Kalk_Bay_Tidal_Pool_Inside_(CS02).xlsm" & date > "2024-02-21" & value_cfu_per_100ml == 1)) |>
  filter(!(file == "63_Muizenberg_Pavilion_(CS16).xlsm" & date >= "2024-02-21" & value_cfu_per_100ml == 31)) |>
  filter(!(file == "64_Muizenberg_Sufing_area-In_Line_with_Bathing_area_(CS34) SSB and CMB.xlsm" & date >= "2024-04-01" & value_cfu_per_100ml == 88)) |>
  filter(!(file == "64_Muizenberg_Sufing_area-In_Line_with_Bathing_area_(CS34).xlsm" & date >= "2024-04-01" & value_cfu_per_100ml == 88)) |>
  filter(!(file == "39_Cosy_Bay_(CN39).xlsm" & date >= "2024-10-31")) |>
  filter(!(file == "39_Oudekraal_(XCN09).xlsm" & date >= "2024-10-31")) |>
  filter(!(file == "466__Millers_Point_tidal_pool_(CS37).xlsm")) |>
  select(-gap)
```

Plotting the data for ecoli shows that there are many horizontal solid lines which show data that are exactly the same at a very high frequency. Having a look at the data shows that there are some sheets where a single value is repeated daily for a long time. Since the values in the rest of the data sheet are often weekly and vary, as can be seen by the random scatter on the plots, these data are certainly not indicative of true data and need to be removed. 

The removal of these data should result in a random scatter of points on the plot.

```{r plot-data}
# Plot the data
ecoli |>
  ggplot(aes(x = date, y = value_cfu_per_100ml, colour = file)) +
  geom_point() +
  theme(legend.position = "none") +
  coord_cartesian(xlim = c(ymd("2022-01-01"), ymd("2025-06-01")))
```

After the removal of the lines in the data, there are still some streaks but at much lower frequencies. These are likely to represent censored values and will be dealt with later.

```{r faecal-plot}
faecal |>
  ggplot(aes(x = date, y = value_cfu_per_100ml, colour = file)) +
  geom_point() +
  theme(legend.position = "none") +
  coord_cartesian(xlim = c(ymd("2014-01-01"), ymd("2025-06-01")))
```

The faecal streptococci data don't seem to have the same solid lines that the E. coli data had. There are also some streaks but these streaks occur at frequencies that can be reasonably expected from weekly sampling. These are also likely to be due to censored values, which will be dealt with in due course.

I want to investigate the source of the values greater than 70000, and the points around 24000.

```{r remove-duplicates}
# Remove duplicate values
ecoli <- ecoli |>
  mutate(
    site_id = str_extract(file, "[:upper:]{2,3}\\d{2}[:upper:]?"),
    full = str_detect(file, "SSB|CMB")
  ) |>
  group_by(date, site_id, full, value_cfu_per_100ml) |>
  slice(1) |> 
  arrange(full, .by_group = TRUE) |>
  ungroup(full) |> 
  slice_tail(n = 1) |>
  ungroup()

faecal <- faecal |>
  mutate(
    site_id = str_extract(file, "[:upper:]{2,3}\\d{2}[:upper:]?"),
    full = str_detect(file, "SSB|CMB")
  ) |>
  group_by(date, site_id, full, value_cfu_per_100ml) |>
  slice(1) |>
  arrange(full, .by_group = TRUE) |>
  ungroup(full) |>
  slice_tail(n = 1) |>
  ungroup()
```

There seem to be some duplicate values in the data. Some of these values naturally duplicated because of the duplication of data in the CMB and SSB duplicate sheets. Values that overlap simply because they appear in both sheets can reasonably be removed. Other values that have the same result for the same site on the same day can reasonably be removed only if there is no precedent for multiple samples on the same day. There are instances where multiple samples are taken on the same day, and therefore this needs to be done carefully so that results are not removed that simply happen to have the same value on the same day for the same site. 

```{r censored-values}
ecoli |>
  ungroup() |>
  arrange(value_cfu_per_100ml)

# Plot the data to see where streaks occur that may indicate the presence of censored values
ecoli |>
  filter(value_cfu_per_100ml < 15 & value_cfu_per_100ml != 1/3 & value_cfu_per_100ml != 10/3) |> 
  ggplot(aes(x = date, y = value_cfu_per_100ml, colour = site_id)) +
  geom_point() +
  scale_y_continuous(breaks = seq(0, 20, 0.5)) +
  scale_x_date(date_breaks = "6 months") +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90))

# Replace the censored values with a character value including the < operator
ecoli <- ecoli |>
  ungroup() |> 
  mutate(censored_value = case_when(
    round(value_cfu_per_100ml, 2) == 0.67 ~ "<2",
    (round(value_cfu_per_100ml, 2) == 0.33 | value_cfu_per_100ml == 0.3) ~ "<1",
    round(value_cfu_per_100ml, 2) == 0.12 ~ "0",
    round(value_cfu_per_100ml, 2) == 0.22 ~ "0",
    round(value_cfu_per_100ml, 2) == 3.33 ~ "<10",
    round(value_cfu_per_100ml, 2) == 16.67 ~ "<50",
    round(value_cfu_per_100ml, 2) == 33.33 ~ "<100",
    round(value_cfu_per_100ml, 2) == 806.67 ~ ">2420",
    round(value_cfu_per_100ml, 2) == 3.48 ~ "4",
    round(value_cfu_per_100ml, 2) == 4.18 ~ "4",
    round(value_cfu_per_100ml, 4) == 2.7789 ~ "3",
    .default = as.character(value_cfu_per_100ml)
    )) |>
  mutate(censored_value = if_else(censored_value == "0", "ND", censored_value))

ecoli |>
  filter(date > "2024-01-01")

# Replace censored values with character value including < operator
faecal <- faecal |>
  ungroup() |> 
  mutate(censored_value = case_when(
    round(value_cfu_per_100ml, 2) == 0.67 ~ "<2",
    (round(value_cfu_per_100ml, 2) == 0.33 | value_cfu_per_100ml == 0.3) ~ "<1",
    round(value_cfu_per_100ml, 3) == 0.033 ~ "<1",
    round(value_cfu_per_100ml, 2) == 0.12 ~ "0",
    round(value_cfu_per_100ml, 2) == 0.22 ~ "0",
    round(value_cfu_per_100ml, 2) == 3.33 ~ "<10",
    round(value_cfu_per_100ml, 2) == 16.67 ~ "<50",
    round(value_cfu_per_100ml, 2) == 33.33 ~ "<100",
    round(value_cfu_per_100ml, 2) == 806.67 ~ ">2420",
    round(value_cfu_per_100ml, 2) == 3.48 ~ "4",
    round(value_cfu_per_100ml, 2) == 4.18 ~ "4",
    round(value_cfu_per_100ml, 2) == 2.48 ~ "2",
    value_cfu_per_100ml == 1.3 ~ "1",
    .default = as.character(value_cfu_per_100ml)
    )) |>
  mutate(censored_value = if_else(censored_value == "0", "ND", censored_value))

faecal |>
  filter(date > "2024-07-01")
```

Censored values can be found both at the lower detection limits and at the upper detection limits. Streaks at the upper limits can also indicate censored values

```{r upper-censored-values-ecoli}
ecoli |>
  ggplot(aes(x = date, y = value_cfu_per_100ml, colour = file)) +
  geom_point() +
  theme(
    legend.position = "none"
  ) +
  coord_cartesian(xlim = c(ymd("2014-01-01"), ymd("2025-01-01"))) +
  scale_x_date(minor_breaks = "1 year")

# Upper limits above 60000 cfu per 100 ml
ecoli |>
  filter(value_cfu_per_100ml > 480, value_cfu_per_100ml < 750, date > "2020-07-01", date < "2022-01-01")

ecoli <- ecoli |>
  mutate(censored_value = case_when(
    value_cfu_per_100ml == 72588 ~ ">24196",
    value_cfu_per_100ml == 72570 ~ ">24190",
    value_cfu_per_100ml == 15000 ~ ">5000",
    value_cfu_per_100ml == 7260 ~ ">2420",
    value_cfu_per_100ml == 600 & date > "2020-04-01" & date < "2021-11-01" ~ ">200",
    .default = censored_value
  ))
```

```{r upper-censored-values-faecal}
faecal |>
  ggplot(aes(x = date, y = value_cfu_per_100ml, colour = file)) +
  geom_point() +
  theme(
    legend.position = "none"
  ) +
  coord_cartesian(xlim = c(ymd("2014-01-01"), ymd("2024-11-01")), ylim = c(0, 20000)) +
  scale_x_date(minor_breaks = "1 year")

# Upper limits above 60000 cfu per 100 ml
faecal |>
  filter(value_cfu_per_100ml == 4500)

faecal <- faecal |>
  mutate(censored_value = case_when(
    value_cfu_per_100ml == 72588 ~ ">24196",
    value_cfu_per_100ml == 24196 ~ ">24196",
    value_cfu_per_100ml == 15000 ~ ">5000",
    value_cfu_per_100ml == 7257 ~ ">2419",
    value_cfu_per_100ml == 450 & date > "2023-08-01" & date < "2024-11-01" ~ ">150",
    value_cfu_per_100ml == 4500 ~ ">1500",
    .default = censored_value
  ))
```

The obvious streaks that were divisible by 3 were divided by three and preceded by the > operator. Other streaks were visible in the 2023 period that are not a consistent value and cannot be easily divided by 3, and are not therefore obvious censored values. These values are of concern, as the frequency indicates a non-random distribution.
Values are quite commonly distributed along 50 unit intervals. From mid-2020 to end of 2021 the value of 600 was very common and was replace during this period with <200 because this is common censored value.

For faecal streptococci, similar censored values were replaced with their character equivalents.

```{r check-censored}
ecoli |>
  distinct(censored_value) |> 
  arrange(censored_value)

faecal |>
  distinct(censored_value) |>
  arrange(censored_value)
```

```{r plot-censored}
ecoli |>
  mutate(censored = str_detect(censored_value, "<|>")) |>
  ggplot(aes(x = date, y = value_cfu_per_100ml, colour = censored)) +
    geom_point() +
  coord_cartesian(xlim = c(ymd("2014-01-01"), ymd("2025-01-01")), ylim = c(0, 80000)) +
  scale_x_date(minor_breaks = "1 year")

faecal |>
  mutate(censored = str_detect(censored_value, "<|>")) |>
  ggplot(aes(x = date, y = value_cfu_per_100ml, colour = censored)) +
    geom_point() +
  coord_cartesian(xlim = c(ymd("2014-01-01"), ymd("2025-01-01")), ylim = c(0, 80000)) +
  scale_x_date(minor_breaks = "1 year")

```

```{r summary-statistics}
# Check summary statistics for ecoli
ecoli |>
  group_by(file) |>
  arrange(file) |>
  summarise(
    min_date = min(date),
    max_date = max(date),
    n = sum(!is.na(value_cfu_per_100ml)),
    min = min(value_cfu_per_100ml, na.rm = TRUE),
    max = max(value_cfu_per_100ml, na.rm = TRUE)
    )

ecoli |>
  filter(value_cfu_per_100ml > 20000)

# Check summary statistics for faecal streptococci
faecal |>
  group_by(file) |>
  arrange(file) |>
  summarise(
    min_date = min(date),
    max_date = max(date),
    n = sum(!is.na(value_cfu_per_100ml)),
    min = min(value_cfu_per_100ml, na.rm = TRUE) |> as.integer(),
    max = max(value_cfu_per_100ml, na.rm = TRUE) |> as.integer()
    )

# Plot by site
faecal |>
  filter(site_id == "CN22") |>
  ggplot(aes(x = date, y = value_cfu_per_100ml)) +
  geom_point()

faecal |>
  filter(value_cfu_per_100ml > 20000)
```

```{r check-sites}
# Create site name from the filename
sites_ecoli <- ecoli |>
  distinct(file, site_id) |>
  mutate(description = file |>
           str_remove("^\\d{1,3}_{1,2}") |>
           str_remove(".xlsm$") |>
           # str_remove("(SSB\\sand\\sCMB)|(CMB\\sand\\sSSB)") |>
           str_remove("\\([:upper:]{2,3}\\d{2}[:upper:]?\\)") |>
           str_replace_all("_", " ") |>
           str_to_upper() |>
           str_trim()) |>
  select(site_id, description) |>
  arrange(site_id)

sites_faecal <- faecal |>
  distinct(file, site_id) |>
  mutate(description = file |>
           str_remove("^\\d{1,3}_{1,2}") |>
           str_remove(".xlsm$") |>
           # str_remove("(SSB\\sand\\sCMB)|(CMB\\sand\\sSSB)") |>
           str_remove("\\([:upper:]{2,3}\\d{2}[:upper:]?\\)") |>
           str_replace_all("_", " ") |>
           str_to_upper() |>
           str_trim()) |>
  select(site_id, description) |>
  arrange(site_id)

# Check which sites are still unique
sites_ecoli |>
  distinct(site_id, description)

sites_faecal |>
  distinct(site_id, description)

# Get current sites
current <- tbl(con, I("coastal.sites")) |>
  collect()

# Compare current sites to previous sites
current |>
  full_join(sites_ecoli) |>
  select(site_id, site_description, description) |>
  rename(description_current = site_description, description_ecoli = description)

# Compare current sites to previous sites
current |>
  full_join(sites_faecal) |>
  select(site_id, site_description, description) |>
  rename(description_current = site_description, description_ecoli = description)
```

# Compare 2023 data between City of Cape Town and Infinity

In 2023/2024 data samples were collected with sample codes that don't match any other sample codes in the monitoring programme. I want to see how well our data corresponds to City of Cape Town data.

```{r read-sabs}
# Read in SABS data from Infinity
sabs <- read_excel("../../Infinity/Infinity Projects - Files/190-56 Coastal WQ/07_Cleaned Data/Coastal WQ Weekly 2023-24_cleaned.xlsx",
           col_names = c(
             "site_description",
             "cct_code",
             "cct_code_updated",
             "updated",
             "coords",
             "area",
             "week",
             "sample_date",
             "temp_c",
             "sabs_code",
             "enterococci_cfu_per_100ml",
             "enterococci_cfu_per_100ml_numeric",
             "sabs_sheet",
             "issues_flagged",
             "comments"
           ), col_types = c(
             "text",
             "text",
             "text",
             "text",
             "text",
             "text",
             "numeric",
             "date",
             "numeric",
             "text",
             "text",
             "numeric",
             "text",
             "text",
             "text"
           ), 
           skip = 1)

# Unique sites and descriptions
sabs |>
  distinct(cct_code, cct_code_updated, site_description) |>
  arrange(cct_code)

# Summarise data
sabs |>
  summary()

# Upload data to database


# Check when coastal data end
tbl(con, I("coastal.results")) |>
  arrange(sample_date) |>
  arrange(sample_date)
```

```{r join-data}
# Create simple table for SABS data that aligns with CCT data
sabs2023 <- sabs |>
  select(cct_code_updated, sample_date, enterococci_cfu_per_100ml, enterococci_cfu_per_100ml_numeric, week) |>
  mutate(cct_code_updated = fct(cct_code_updated)) |>
  filter(!is.na(sample_date)) |>
  rename(site_id = cct_code_updated) |>
  select(1, 2, 3)

# How many records for each site?
filter(sabs2023) |>
  group_by(site_id) |>
  count() |>
  arrange(-n)

# Create a table with data for the same period as the SABS data
faec2023_full <- faecal |>
  filter(date >= "2023-10-03" & date <= "2024-11-06" & full) |>
  rename(sample_date = date, enterococci_cfu_per_100ml = censored_value) |>
  select(site_id, sample_date, enterococci_cfu_per_100ml) |>
  mutate(enterococci_cfu_per_100ml = if_else(enterococci_cfu_per_100ml == "<1", "ND", enterococci_cfu_per_100ml))

faec2023_ssb <- faecal |>
  filter(date >= "2023-10-03" & date <= "2024-11-06" & !full) |>
  rename(sample_date = date, enterococci_cfu_per_100ml = censored_value) |>
  select(site_id, sample_date, enterococci_cfu_per_100ml) |>
  mutate(enterococci_cfu_per_100ml = if_else(enterococci_cfu_per_100ml == "<1", "ND", enterococci_cfu_per_100ml))

# How many sites for each?
sabs2023 |>
  distinct(site_id) # 36 sites

faec2023_full |>
  distinct(site_id) # 20 sites

faec2023_ssb |>
  distinct(site_id) # 62 sites

# Find sites that do not have a combined spreadsheet
sites_not_combined <- setdiff(distinct(faec2023_ssb, site_id), distinct(faec2023_full, site_id)) |> pull(1)

# Number of records
nrow(sabs2023) # 1549
nrow(faec2023) # 1877

# Combine the three data sets
faec2023_full |> mutate(dataset = "SSB + CMB") |>
  bind_rows(sabs2023 |> mutate(dataset = "SABS")) |>
  bind_rows(faec2023_ssb |> mutate(dataset = "SSB")) |>
  arrange(sample_date, site_id, dataset, enterococci_cfu_per_100ml) |>
  filter(!(site_id %in% sites_not_combined & dataset == "SSB"))

sabs |>
  distinct(cct_code, cct_code_updated, site_description) |>
  arrange(cct_code)

# Handle censored values the same way
sabs <- sabs |>
  mutate(numeric = case_when(
    str_detect(enterococci_cfu_per_100ml, "<") ~ 0,
    str_detect(enterococci_cfu_per_100ml, ">") ~ as.numeric(str_remove(enterococci_cfu_per_100ml, ">")) * 3,
    str_detect(enterococci_cfu_per_100ml, "ND") ~ 0,
    .default = as.numeric(enterococci_cfu_per_100ml)
  ),
  sample_date = date(sample_date))

faec2023 <- faec2023 |>
  mutate(numeric = case_when(
    str_detect(enterococci_cfu_per_100ml, "<") ~ 0,
    str_detect(enterococci_cfu_per_100ml, ">") ~ as.numeric(str_remove(enterococci_cfu_per_100ml, ">")) * 3,
    str_detect(enterococci_cfu_per_100ml, "ND") ~ 0,
    .default = as.numeric(enterococci_cfu_per_100ml)
  ))

ggplot() +
  geom_point(data = sabs, aes(x = sample_date, y = numeric, colour = "SABS Data")) +
  geom_point(data = faec2023, aes(x = sample_date, y = numeric, colour = "CCT Data")) +
  theme(
    legend.position = "none"
  )

sabs |>
  full_join(faec2023, by = c("sample_date" = "date", "cct_code_updated" = "site_id"), keep = TRUE) |>
  select(cct_code_updated, sample_date, site_id, date, enterococci_cfu_per_100ml, value_cfu_per_100ml) |>
  filter(!if_any(c(cct_code_updated, site_id), is.na))

faecal |>
  filter(value_cfu_per_100ml > 60000)

filter(faecal, date == "2024-10-09" & site_id == "CN22")

```

Possible scenarios:
1. Exists in CCT SSB + CMB data + not in CCT SSB data + not in SABS data (SSB data that is not the SABS data)(sample date wrong in Excel)
2. Exists in CCT SSB + CMB data + not in CCT SSB data + exists in SABS data (SSB data that is from SABS data)(ideal)
3. No SSB + CMB sheet + not in CCT SSB + exists in SABS (No sheet for weekly samples)
4. No sheet in CCT + exists in SABS (site code not part of larger monitoring programme, doesn't correspond to current site code)
5. Not in CCT SSB + CMB + not in CCT SSB data + exists in SABS data (not captured)(not ideal)
6. Exists in CCT SSB + CMB + exists in CCT SSB + not in SABS data (SSB Data)(ideal)

The data set from Infinity has 1565 records, and for the same period, the CCT data set has 1877 records.
There are 569 combinations of site and date that exist in both data sets. There is some agreement between the values - many of the values of 3.33333 or 0 are ND values in the Infinity data. Some values do not agree between 
SABS data has 1565 records

# Hazen Method

```{r test-hazen}
hazen_annual <- results |>
  group_by(year = year(date)) |>
  summarise(
    min_date = min(date),
    max_date = max(date),
    n_ecoli = sum(!is.na(ecoli_cfu_per_100ml)),
    hazen95_ecoli = quantile(ecoli_cfu_per_100ml, 0.95, type = 5, na.rm = TRUE),
    hazen90_ecoli = quantile(ecoli_cfu_per_100ml, 0.9, type = 5, na.rm = TRUE),
    hazen_category_ecoli = case_when(
      n_ecoli < 10 ~ "Not enough samples",
      hazen95_ecoli <= 100 ~ "Excellent",
      hazen95_ecoli <= 200 ~ "Good",
      hazen95_ecoli > 200 & hazen90_ecoli > 185 ~ "Poor",
      hazen95_ecoli > 200 & hazen90_ecoli < 185 ~ "Sufficient"
  ),
    n_faecal = sum(!is.na(faecal_streptococci_cfu_per_100ml)),
    hazen95_faecal = quantile(faecal_streptococci_cfu_per_100ml, 0.95, type = 5, na.rm = TRUE),
    hazen90_faecal = quantile(faecal_streptococci_cfu_per_100ml, 0.9, type = 5, na.rm = TRUE),
  hazen_category_faecal = case_when(
      n_faecal < 10 ~ "Not enough samples",
      hazen95_faecal <= 100 ~ "Excellent",
      hazen95_faecal <= 200 ~ "Good",
      hazen95_faecal > 200 & hazen90_faecal > 185 ~ "Poor",
      hazen95_faecal > 200 & hazen90_faecal < 185 ~ "Sufficient"
  )
  ) |>
  mutate(hazen_category_ecoli = fct(hazen_category_ecoli, levels = c("Not enough samples", "Poor", "Sufficient", "Good", "Excellent"))) |>
  mutate(hazen_category_faecal = fct(hazen_category_faecal, levels = c("Not enough samples", "Poor", "Sufficient", "Good", "Excellent")))
```

```{r test-hazen}
hazen_annual_site <- results |>
  group_by(year = year(date), site_id) |>
  summarise(
    min_date = min(date),
    max_date = max(date),
    n_ecoli = sum(!is.na(ecoli_cfu_per_100ml)),
    hazen95_ecoli = quantile(ecoli_cfu_per_100ml, 0.95, type = 5, na.rm = TRUE),
    hazen90_ecoli = quantile(ecoli_cfu_per_100ml, 0.9, type = 5, na.rm = TRUE),
    hazen_category_ecoli = case_when(
      n_ecoli < 10 ~ "Not enough samples",
      hazen95_ecoli <= 100 ~ "Excellent",
      hazen95_ecoli <= 200 ~ "Good",
      hazen95_ecoli > 200 & hazen90_ecoli > 185 ~ "Poor",
      hazen95_ecoli > 200 & hazen90_ecoli < 185 ~ "Sufficient"
  ),
    n_faecal = sum(!is.na(faecal_streptococci_cfu_per_100ml)),
    hazen95_faecal = quantile(faecal_streptococci_cfu_per_100ml, 0.95, type = 5, na.rm = TRUE),
    hazen90_faecal = quantile(faecal_streptococci_cfu_per_100ml, 0.9, type = 5, na.rm = TRUE),
  hazen_category_faecal = case_when(
      n_faecal < 10 ~ "Not enough samples",
      hazen95_faecal <= 100 ~ "Excellent",
      hazen95_faecal <= 200 ~ "Good",
      hazen95_faecal > 200 & hazen90_faecal > 185 ~ "Poor",
      hazen95_faecal > 200 & hazen90_faecal < 185 ~ "Sufficient"
  )
  ) |>
  mutate(hazen_category_ecoli = fct(hazen_category_ecoli, levels = c("Not enough samples", "Poor", "Sufficient", "Good", "Excellent"))) |>
  mutate(hazen_category_faecal = fct(hazen_category_faecal, levels = c("Not enough samples", "Poor", "Sufficient", "Good", "Excellent")))
```

```{r plot-hazen}
hazen_annual |>
  ggplot(aes(x = year)) +
  labs(x = "Year") +
  geom_tile(aes(fill = hazen_category_ecoli)) +
  scale_fill_brewer(name = "Water Quality Category", type = "div", palette = "RdYlGn")

RColorBrewer::display.brewer.all()
```

```{r}
sites |>
  inner_join(hazen_annual)
```

# LIMS Data

```{r read-lims-data}
lims <- read_excel("01 Input Data/Coastal Water Quality Data/LIMS Data/LIMS Data.xlsm",
           col_names = c("sample_date", "site_id", "lab_owner", "analysis", "enterococci_cfu_per_100ml", "sample_number"),
           col_types = c("date", "text", "text", "text", "text", "numeric"),
           skip = 1)

lims |>
  filter(str_detect(enterococci_cfu_per_100ml, "<")) |>
  distinct(enterococci_cfu_per_100ml)

lims |>
  filter(str_detect(enterococci_cfu_per_100ml, ">")) |>
  distinct(enterococci_cfu_per_100ml)

lims |>
  filter(str_detect(enterococci_cfu_per_100ml, "2419")) |>
  ggplot(aes(x = sample_date)) +
  geom_density()

lims |>
  mutate(sample_date = date(sample_date),
         enterococci_cfu_per_100ml_numeric = case_when(
           str_detect(enterococci_cfu_per_100ml, "<")
         ))
```

